{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mattywonger/ESM3_tutorial/blob/main/ESM3_tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-WMd1_DXEeR"
      },
      "source": [
        "Tokenization refers to the process of converting high-dimensional data into numerical representations.\n",
        "\n",
        "For instance, in GPT-3 model, sentences are broken down into individual words. Individual words are further broken down into sub-words, also known as tokens. Then there is a dictionary that associates a numerical value with a token. For example, the word \"My\" might correspond to the number 1 and the word \"name\" corresponds to the number 16.\n",
        "\n",
        "Then the phrase my name might be represented with the matrix (1,16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lidhv1xSYmD4"
      },
      "source": [
        "In the ESM3 model, there are 6 inputs that needs to be tokenized:\n",
        "\n",
        "\n",
        "1.   Sequence\n",
        "2.   Structure\n",
        "3.   Function Annotation\n",
        "4.   Secondary Structure\n",
        "5.   Surface Area Solvent Accessible (SASA)\n",
        "6.   Residue Annotation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81xjHbLrVYIJ"
      },
      "source": [
        "# Sequence Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4dlhCEteSPG",
        "outputId": "e88f1f6f-6cc7-4c41-af48-a25c3923c84a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-24.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRTVDmCdeVEW",
        "outputId": "f2f19ba4-3826-4786-8d8c-e94156805728"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFck_RH1bEdI",
        "outputId": "92f1eaff-2779-403f-b80f-ebbccbf3077e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers) (0.23.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers) (2024.7.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RmDJFiJmTTDO"
      },
      "outputs": [],
      "source": [
        "from typing import Protocol, runtime_checkable\n",
        "\n",
        "\n",
        "@runtime_checkable\n",
        "class EsmTokenizerBase(Protocol):\n",
        "    def encode(self, *args, **kwargs):\n",
        "        ...\n",
        "\n",
        "    def decode(self, *args, **kwargs):\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def mask_token(self) -> str:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def mask_token_id(self) -> int:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def bos_token(self) -> str:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self) -> int:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def eos_token(self) -> str:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def eos_token_id(self) -> int:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def pad_token(self) -> str:\n",
        "        ...\n",
        "\n",
        "    @property\n",
        "    def pad_token_id(self) -> int:\n",
        "        ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1vJpSwT8Z6GH"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import BPE\n",
        "from tokenizers.processors import TemplateProcessing\n",
        "from transformers import PreTrainedTokenizerFast\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iUhjXjPfZk1o"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_VOCAB = [\n",
        "    \"<cls>\", \"<pad>\", \"<eos>\", \"<unk>\",\n",
        "    \"L\", \"A\", \"G\", \"V\", \"S\", \"E\", \"R\", \"T\", \"I\", \"D\", \"P\", \"K\",\n",
        "    \"Q\", \"N\", \"F\", \"Y\", \"M\", \"H\", \"W\", \"C\", \"X\", \"B\", \"U\", \"Z\",\n",
        "    \"O\", \".\", \"-\", \"|\",\n",
        "    \"<mask>\",\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "O7cT-6SpZ1bW"
      },
      "outputs": [],
      "source": [
        "class EsmSequenceTokenizer(PreTrainedTokenizerFast, EsmTokenizerBase):\n",
        "    \"\"\"\n",
        "    Constructs an ESM tokenizer.\n",
        "    \"\"\"\n",
        "\n",
        "    model_input_names = [\"sequence_tokens\", \"attention_mask\"]\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        unk_token=\"<unk>\",\n",
        "        cls_token=\"<cls>\",\n",
        "        pad_token=\"<pad>\",\n",
        "        mask_token=\"<mask>\",\n",
        "        eos_token=\"<eos>\",\n",
        "        chainbreak_token=\"|\",\n",
        "        **kwargs,\n",
        "    ):\n",
        "        all_tokens = SEQUENCE_VOCAB\n",
        "        token_to_id = {tok: ind for ind, tok in enumerate(all_tokens)}\n",
        "\n",
        "        # a character-level tokenizer is the same as BPE with no token merges\n",
        "        bpe = BPE(token_to_id, merges=[], unk_token=unk_token)\n",
        "        tokenizer = Tokenizer(bpe)\n",
        "        special_tokens = [cls_token, pad_token, mask_token, eos_token, chainbreak_token]\n",
        "        additional_special_tokens = [chainbreak_token]\n",
        "\n",
        "        tokenizer.add_special_tokens(\n",
        "            special_tokens,\n",
        "        )\n",
        "\n",
        "        # This is where we configure the automatic addition of special tokens when we call\n",
        "        # tokenizer(text, add_special_tokens=True). Note that you can also configure how two\n",
        "        # sequences are merged if you want.\n",
        "        tokenizer.post_processor = TemplateProcessing(  # type: ignore\n",
        "            single=\"<cls> $A <eos>\",\n",
        "            special_tokens=[\n",
        "                (\"<cls>\", tokenizer.token_to_id(\"<cls>\")),\n",
        "                (\"<eos>\", tokenizer.token_to_id(\"<eos>\")),\n",
        "            ],\n",
        "        )\n",
        "        super().__init__(\n",
        "            tokenizer_object=tokenizer,\n",
        "            unk_token=unk_token,\n",
        "            cls_token=cls_token,\n",
        "            pad_token=pad_token,\n",
        "            mask_token=mask_token,\n",
        "            eos_token=eos_token,\n",
        "            additional_special_tokens=additional_special_tokens,\n",
        "            **kwargs,\n",
        "        )\n",
        "\n",
        "    # These are a footgun, we never use the `bos` token anywhere so we're just overriding it here.\n",
        "    @property\n",
        "    def bos_token(self):\n",
        "        return self.cls_token\n",
        "\n",
        "    @property\n",
        "    def bos_token_id(self):\n",
        "        return self.cls_token_id\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BC74kPAKaTXy",
        "outputId": "31f1a603-5a85-4824-bb37-eb01ad4edc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71\n",
            "71\n",
            "torch.Size([2, 73])\n"
          ]
        }
      ],
      "source": [
        "sequence_tokenizer = EsmSequenceTokenizer()\n",
        "sequence1 = \"AGCTGACCTGAAGTCCGATCGTAACTGGCATAGCGTATGCCGTACGTAGGCTACGATCGATAGCTGACCGT\"\n",
        "sequence2 = \"ATGCTAGCTGACCGTACGTTAGCTAGCTGATCGTAGCTAGTCGATCGTAGCTGATCGTAGCTAGCTAGCTA\"\n",
        "print(len(sequence1))\n",
        "print(len(sequence2))\n",
        "sequence_tokens1 = sequence_tokenizer.encode(\n",
        "        sequence1, add_special_tokens=True\n",
        "    )\n",
        "sequence_tokens2 = sequence_tokenizer.encode(\n",
        "        sequence2, add_special_tokens=True\n",
        "    )\n",
        "sequence_2d = torch.stack((torch.tensor(sequence_tokens1), torch.tensor(sequence_tokens2)), dim=0)\n",
        "print(sequence_2d.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Encoding\n"
      ],
      "metadata": {
        "id": "ZW5xFwOflXdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch"
      ],
      "metadata": {
        "id": "s8hCqTTCqOWp"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_embed = nn.Embedding(64, 1536)"
      ],
      "metadata": {
        "id": "q52NwJQC0QHs"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_after_embed = sequence_embed(sequence_2d)"
      ],
      "metadata": {
        "id": "dsxxhr1N0mWQ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence_after_embed.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kn2xfRI2eEW",
        "outputId": "1a8fe21c-5e5c-40cd-949f-4084c6af5a49"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 73, 1536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequence_after_embed[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjTNEtsBhPvM",
        "outputId": "c5255eb8-2d3f-44a2-d85e-e2f08818a50d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([73, 1536])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Inputs to the transformer architecture must be\n",
        "\n",
        "> \"The input tensor of shape (batch_size, sequence_length, d_model).\"\n",
        "\n",
        "\n",
        ">  sequence_id (torch.Tensor): The sequence ID tensor of shape (batch_size, sequence_length).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jg73_8NbdiSb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1tZIcbycMW1"
      },
      "source": [
        "# Transformer architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VFB2MDHQdngs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import functools\n",
        "!pip install einops\n",
        "import torch.nn.functional as F\n",
        "import einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qe2T9D_cVlFX",
        "outputId": "fe6cb220-787c-4003-fe36-f0ed13621ce1"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj9PWD3Xcfqx"
      },
      "source": [
        "### Multi-head attention class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        bias: bool = False,\n",
        "        qk_layernorm: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "\n",
        "        self.d_head = self.d_model // self.n_heads\n",
        "        self.layernorm_qkv = nn.Sequential(\n",
        "            nn.LayerNorm(d_model), nn.Linear(d_model, d_model * 3, bias=bias)\n",
        "        )\n",
        "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
        "\n",
        "        if qk_layernorm:\n",
        "            self.q_ln = nn.LayerNorm(d_model, bias=bias)\n",
        "            self.k_ln = nn.LayerNorm(d_model, bias=bias)\n",
        "        else:\n",
        "            self.q_ln = nn.Identity()\n",
        "            self.k_ln = nn.Identity()\n",
        "    def forward(self,x,seq_id):\n",
        "        qkv_BLD3 = self.layernorm_qkv(x)\n",
        "        query_BLD, key_BLD, value_BLD = torch.chunk(qkv_BLD3, 3, dim=-1)\n",
        "        query_BLD, key_BLD = self.q_ln(query_BLD), self.k_ln(key_BLD)\n",
        "        #query_BLD, key_BLD = self._apply_rotary(query_BLD, key_BLD) ##WILL IMPLEMENT\n",
        "\n",
        "        n_heads = self.n_heads\n",
        "        reshaper = functools.partial(\n",
        "            einops.rearrange, pattern=\"b s (h d) -> b h s d\", h=n_heads\n",
        "        )\n",
        "\n",
        "        query_BHLD, key_BHLD, value_BHLD = map(\n",
        "            reshaper, (query_BLD, key_BLD, value_BLD)\n",
        "        )\n",
        "\n",
        "        # Where True, enable participation in attention.\n",
        "        mask_BLL = seq_id.unsqueeze(-1) == seq_id.unsqueeze(-2)\n",
        "        mask_BHLL = mask_BLL.unsqueeze(1)\n",
        "\n",
        "        context_BHLD = F.scaled_dot_product_attention(\n",
        "            query_BHLD, key_BHLD, value_BHLD, mask_BHLL\n",
        "        )\n",
        "        context_BLD = einops.rearrange(context_BHLD, \"b h s d -> b s (h d)\")\n",
        "        return self.out_proj(context_BLD)"
      ],
      "metadata": {
        "id": "VYevRGPK4NAl"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SwiGLU activation function"
      ],
      "metadata": {
        "id": "SmhwvOq8OnUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SwiGLU(nn.Module):\n",
        "    \"\"\"\n",
        "    SwiGLU activation function as an nn.Module, allowing it to be used within nn.Sequential.\n",
        "    This module splits the input tensor along the last dimension and applies the SiLU (Swish)\n",
        "    activation function to the first half, then multiplies it by the second half.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SwiGLU, self).__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x1, x2 = x.chunk(2, dim=-1)\n",
        "        return F.silu(x1) * x2"
      ],
      "metadata": {
        "id": "O5in8HLN48kC"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chunk function splits the input into two halves along the last dimension.\n",
        "\n",
        "Then SiLU activation is applied to X1 and multiplied element-wise with"
      ],
      "metadata": {
        "id": "CBLhPel2QJnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def swiglu_correction_fn(expansion_ratio: float, d_model: int) -> int:\n",
        "    # set hidden dimesion to nearest multiple of 256 after expansion ratio\n",
        "    return int(((expansion_ratio * d_model) + 255) // 256 * 256)"
      ],
      "metadata": {
        "id": "4NTSU8i3PWHt"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def swiglu_ln_ffn(d_model: int, expansion_ratio: float, bias: bool):\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(d_model),\n",
        "        nn.Linear(\n",
        "            d_model, swiglu_correction_fn(expansion_ratio, d_model) * 2, bias=bias\n",
        "        ),\n",
        "        SwiGLU(),\n",
        "        nn.Linear(swiglu_correction_fn(expansion_ratio, d_model), d_model, bias=bias),\n",
        "    )"
      ],
      "metadata": {
        "id": "2CsL2bXA4shU"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Transformer Block"
      ],
      "metadata": {
        "id": "pdj7S-9IPbN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnifiedTransformerBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        v_heads: int | None = None,\n",
        "        bias: bool = False,\n",
        "        expansion_ratio: float = 4.0,\n",
        "        residue_scaling_factor: float = 1,\n",
        "        mask_and_zero_frameless: bool = False,\n",
        "        qk_layernorm: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(\n",
        "                d_model, n_heads, bias, qk_layernorm=qk_layernorm\n",
        "            )\n",
        "        self.ffn = swiglu_ln_ffn(d_model, expansion_ratio, bias)\n",
        "        self.scaling_factor = residue_scaling_factor\n",
        ""
      ],
      "metadata": {
        "id": "bPgftrxK4DTG"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "JTqJwO5qciIm"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class TransformerStack(nn.Module):\n",
        "    \"\"\"\n",
        "    A stack of transformer blocks used in the ESM-3 model. Each block is a UnifiedTransformerBlock,\n",
        "    which can either be geometric attention or standard multi-head attention.\n",
        "\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        d_model: int,\n",
        "        n_heads: int,\n",
        "        v_heads: int | None,\n",
        "        n_layers: int,\n",
        "        n_layers_geom: int = 1,\n",
        "        scale_residue: bool = True,\n",
        "        mask_and_zero_frameless: bool = False,\n",
        "        bias: bool = False,\n",
        "        qk_layernorm: bool = True,\n",
        "        ffn_type: str = \"swiglu\",  # swiglu | gelu\n",
        "        expansion_ratio: float = 8 / 3,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.ModuleList(\n",
        "            [\n",
        "                UnifiedTransformerBlock(\n",
        "                    d_model,\n",
        "                    n_heads,\n",
        "                    v_heads=v_heads,\n",
        "                    residue_scaling_factor=(\n",
        "                        math.sqrt(n_layers / 36) if scale_residue else 1.0\n",
        "                    ),\n",
        "                    expansion_ratio=expansion_ratio,\n",
        "                    mask_and_zero_frameless=mask_and_zero_frameless,\n",
        "                    bias=bias,\n",
        "                    qk_layernorm=qk_layernorm,\n",
        "                )\n",
        "                for i in range(n_layers)\n",
        "            ]\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(d_model, bias=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = TransformerStack(\n",
        "            1536,\n",
        "            24,\n",
        "            64,\n",
        "            48,\n",
        "        )"
      ],
      "metadata": {
        "id": "ZjILxeEr37G6"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count = 0\n",
        "for block in transformer.blocks:\n",
        "    count += 1\n",
        "    r1 = block.attn(sequence_after_embed,None)\n",
        "    r3 =  block.ffn(r1)/block.scaling_factor\n",
        "    x = r1+r3\n",
        "    print(x.shape)\n",
        "\n",
        "\n",
        "print(count)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "gGD2SY8s6gQY",
        "outputId": "bf037c4d-0c5b-497d-b496-98a6f82744df"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'unsqueeze'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-85-a40051cc1524>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_after_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mr3\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mffn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaling_factor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mr3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-74-9c63209e8a8b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_id)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m# Where True, enable participation in attention.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mmask_BLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mseq_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mmask_BHLL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_BLL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'unsqueeze'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = nn.Linear(1536,1536)\n"
      ],
      "metadata": {
        "id": "_zCW38kw8O2Q"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_BLD, key_BLD, value_BLD = torch.chunk(qkv_BLD3, 3, dim=-1)\n",
        "query_BLD, key_BLD = self.q_ln(query_BLD), self.k_ln(key_BLD)\n",
        "query_BLD, key_BLD = self._apply_rotary(query_BLD, key_BLD)\n",
        "\n",
        "\n",
        "\n",
        "query_BHLD, key_BHLD, value_BHLD = map(\n",
        "            reshaper, (query_BLD, key_BLD, value_BLD)\n",
        "        )\n",
        "\n",
        "        # Where True, enable participation in attention.\n",
        "        mask_BLL = seq_id.unsqueeze(-1) == seq_id.unsqueeze(-2)\n",
        "        mask_BHLL = mask_BLL.unsqueeze(1)\n",
        "\n",
        "        context_BHLD = F.scaled_dot_product_attention(\n",
        "            query_BHLD, key_BHLD, value_BHLD, mask_BHLL\n",
        "        )\n",
        "        context_BLD = einops.rearrange(context_BHLD, \"b h s d -> b s (h d)\")\n",
        "        return self.out_proj(context_BLD)"
      ],
      "metadata": {
        "id": "malfeYcA94Nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPieqdN3hmeZhF7C8t5qQrf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}